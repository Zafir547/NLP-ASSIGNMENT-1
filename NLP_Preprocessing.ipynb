{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2103370",
   "metadata": {},
   "source": [
    "# **NLP Preprocessing Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cea447",
   "metadata": {},
   "source": [
    "## **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688f95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21179b3",
   "metadata": {},
   "source": [
    "## **Download required NLTK data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e2deabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zafir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zafir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zafir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86332b9",
   "metadata": {},
   "source": [
    "## **Paragraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddc5de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Text:\n",
      "\n",
      "\n",
      "<h1>sooo yesterday!!!</h1> i taught the <b>students</b> about <i>nlp preprocessing</i> , yeah yeah like lower  CASE , stop words, and punctuation ... oh wow!! what a  mess mess... there were about 21 or 22 students 999 888 maybe ?? not sure ü§î anyway  we talked and talked  and talked about commas, dots, dots... more dots..... ??? question marks, semi;colons;;; and other stuff !!!!  \n",
      "then some <div> of </div> them said like ‚Äúsir why we remove punctuation 123 456 789 numbers , html tags <p></p> and spaces spaces   and more  spaces ???‚Äù i said because it makes <code>the data clean clean clean</code> oh yes YES yes.  \n",
      "we also found sooo many stopwords like \"the\" \"a\" \"an\" \"is\" \"of\" \"in\" \"to\" \"on\" \"for\" <span>and</span>  \"that\" \"by\" oh nooo, and ugh  repeated words everywhere everywhere everywhere !!!  \n",
      "sometimes  the  text  had  BIG  letters, small letters, Missing letters, ...  punctuation,  punctuation,  punctuation!!!  üò©üò© lowercase is important, uppercase looks  BAD BAD BAD!!!  \n",
      "and  we  found  multiple   spaces     like     this     one     and    even     worse    ones.   some sentences ended ??? without any reason ... others ended !!!!!!! or ........ or 1234567890 randomly inserted in middle of text üòê .  \n",
      "students were like <a href=\"#\">sir we tired</a> üò¥ but i said ‚Äúno!! clean the DATA!! remove html tags <img> <body> <head>, remove punctuation!!!!! remove stopwords, remove numbers 000111222333, remove emojis üòÖüòÇüêç, remove extra spaces!!! ‚Äù  \n",
      "then one student wrote: REMOVE   STOPWORDS  PLEASE!!!!!! PLEASE  PLEASE!!!! in ALL CAPS üò≠üò≠üò≠ and forgot half of the text lol lol lol.  \n",
      "we used nltk , regex , and re.sub() and .split() and join() join() join()  functions ... oh so many ( ) ( ) [ ] { } symbols; ; ; ; !!!  \n",
      "btw <html> we </html> also found that lowercase makes the <p>text</p> more readable readable readable readable readable !!! not LIKE THIS or LIKE THAT .  \n",
      "<article> after </article> finishing <section> class </section> , we all laughed, smiled, cleaned cleaned data, removed numbers(12345 67890), stopwords stopwords stopwords stopwords, and extra  spaces!!!  \n",
      "finally the dataset looked better better better better, before it was a big mess mess mess mess mess full of tags <div> <meta> <h1> </h1> <title> broken html <br> <hr> </hr> punctuation!!!  \n",
      "some text was like:  <b>this.. is.. just.. messy.. text..!!!</b>  others like  what??? why???? no idea... anyway.  \n",
      "cleaning text is so boring boring boring but also fun fun fun!!! it makes model smarter, faster, better, clearer, not slower slower slower üòúüòúüòú.  \n",
      "and i said again again again, ‚Äúdon‚Äôt forget to remove punctuation, stopwords, numbers, emojis, and html tags!!!‚Äù  \n",
      "then they all said ‚Äúyes sir yes sir‚Äù three times three times three times.  \n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "<h1>sooo yesterday!!!</h1> i taught the <b>students</b> about <i>nlp preprocessing</i> , yeah yeah like lower  CASE , stop words, and punctuation ... oh wow!! what a  mess mess... there were about 21 or 22 students 999 888 maybe ?? not sure ü§î anyway  we talked and talked  and talked about commas, dots, dots... more dots..... ??? question marks, semi;colons;;; and other stuff !!!!  \n",
    "then some <div> of </div> them said like ‚Äúsir why we remove punctuation 123 456 789 numbers , html tags <p></p> and spaces spaces   and more  spaces ???‚Äù i said because it makes <code>the data clean clean clean</code> oh yes YES yes.  \n",
    "we also found sooo many stopwords like \"the\" \"a\" \"an\" \"is\" \"of\" \"in\" \"to\" \"on\" \"for\" <span>and</span>  \"that\" \"by\" oh nooo, and ugh  repeated words everywhere everywhere everywhere !!!  \n",
    "sometimes  the  text  had  BIG  letters, small letters, Missing letters, ...  punctuation,  punctuation,  punctuation!!!  üò©üò© lowercase is important, uppercase looks  BAD BAD BAD!!!  \n",
    "and  we  found  multiple   spaces     like     this     one     and    even     worse    ones.   some sentences ended ??? without any reason ... others ended !!!!!!! or ........ or 1234567890 randomly inserted in middle of text üòê .  \n",
    "students were like <a href=\"#\">sir we tired</a> üò¥ but i said ‚Äúno!! clean the DATA!! remove html tags <img> <body> <head>, remove punctuation!!!!! remove stopwords, remove numbers 000111222333, remove emojis üòÖüòÇüêç, remove extra spaces!!! ‚Äù  \n",
    "then one student wrote: REMOVE   STOPWORDS  PLEASE!!!!!! PLEASE  PLEASE!!!! in ALL CAPS üò≠üò≠üò≠ and forgot half of the text lol lol lol.  \n",
    "we used nltk , regex , and re.sub() and .split() and join() join() join()  functions ... oh so many ( ) ( ) [ ] { } symbols; ; ; ; !!!  \n",
    "btw <html> we </html> also found that lowercase makes the <p>text</p> more readable readable readable readable readable !!! not LIKE THIS or LIKE THAT .  \n",
    "<article> after </article> finishing <section> class </section> , we all laughed, smiled, cleaned cleaned data, removed numbers(12345 67890), stopwords stopwords stopwords stopwords, and extra  spaces!!!  \n",
    "finally the dataset looked better better better better, before it was a big mess mess mess mess mess full of tags <div> <meta> <h1> </h1> <title> broken html <br> <hr> </hr> punctuation!!!  \n",
    "some text was like:  <b>this.. is.. just.. messy.. text..!!!</b>  others like  what??? why???? no idea... anyway.  \n",
    "cleaning text is so boring boring boring but also fun fun fun!!! it makes model smarter, faster, better, clearer, not slower slower slower üòúüòúüòú.  \n",
    "and i said again again again, ‚Äúdon‚Äôt forget to remove punctuation, stopwords, numbers, emojis, and html tags!!!‚Äù  \n",
    "then they all said ‚Äúyes sir yes sir‚Äù three times three times three times.  \n",
    "\"\"\"\n",
    "\n",
    "print(\"Orginal Text:\\n\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb1f96",
   "metadata": {},
   "source": [
    "## **Lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0599f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Lowercase:\n",
      " \n",
      "<h1>sooo yesterday!!!</h1> i taught the <b>students</b> about <i>nlp preprocessing</i> , yeah yeah like lower  case , stop words, and punctuation ... oh wow!! what a  mess mess... there were about 21 or 22 students 999 888 maybe ?? not sure ü§î anyway  we talked and talked  and talked about commas, d ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(\"After Lowercase:\\n\", text[:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d809dc",
   "metadata": {},
   "source": [
    "## **Remove URLs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6195634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'https?://\\S+|www\\.\\s+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b7c5a",
   "metadata": {},
   "source": [
    "## **Remove HTML tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6da39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'<.*?>', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57399f1f",
   "metadata": {},
   "source": [
    "## **Remove emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353999e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = emoji.replace_emoji(text, replace='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a868053a",
   "metadata": {},
   "source": [
    "## **Remove Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de4d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\d+(\\.\\d+)?', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edba0aa",
   "metadata": {},
   "source": [
    "## **Remove Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dbba143",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da0399",
   "metadata": {},
   "source": [
    "## **Remove Extra Whitespaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2cfe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Basic Cleaning:\n",
      "\n",
      "sooo yesterday i taught the students about nlp preprocessing yeah yeah like lower case stop words and punctuation oh wow what a mess mess there were about or students maybe not sure anyway we talked and talked and talked about commas dots dots more dots question marks semicolons and other stuff then some of them said like ‚Äúsir why we remove punctuation numbers html tags and spaces spaces and more spaces ‚Äù i said because it makes the data clean clean clean oh yes yes yes we also found sooo many stopwords like the a an is of in to on for and that by oh nooo and ugh repeated words everywhere everywhere everywhere sometimes the text had big letters small letters missing letters punctuation punctuation punctuation lowercase is important uppercase looks bad bad bad and we found multiple spaces like this one and even worse ones some sentences ended without any reason others ended or or randomly inserted in middle of text students were like sir we tired but i said ‚Äúno clean the data remove html tags remove punctuation remove stopwords remove numbers remove emojis remove extra spaces ‚Äù then one student wrote remove stopwords please please please in all caps and forgot half of the text lol lol lol we used nltk regex and resub and split and join join join functions oh so many symbols btw we also found that lowercase makes the text more readable readable readable readable readable not like this or like that after finishing class we all laughed smiled cleaned cleaned data removed numbers stopwords stopwords stopwords stopwords and extra spaces finally the dataset looked better better better better before it was a big mess mess mess mess mess full of tags broken html punctuation some text was like this is just messy text others like what why no idea anyway cleaning text is so boring boring boring but also fun fun fun it makes model smarter faster better clearer not slower slower slower and i said again again again ‚Äúdon‚Äôt forget to remove punctuation stopwords numbers emojis and html tags‚Äù then they all said ‚Äúyes sir yes sir‚Äù three times three times three times\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "print(\"After Basic Cleaning:\\n\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2597e",
   "metadata": {},
   "source": [
    "## **Remove Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "891f0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Removing Stopwords:\n",
      "\n",
      "sooo yesterday taught students nlp preprocessing yeah yeah like lower case stop words punctuation oh wow mess mess students maybe sure anyway talked talked talked commas dots dots dots question marks semicolons stuff said like ‚Äúsir remove punctuation numbers html tags spaces spaces spaces ‚Äù said makes data clean clean clean oh yes yes yes also found sooo many stopwords like oh nooo ugh repeated words everywhere everywhere everywhere sometimes text big letters small letters missing letters punctuation punctuation punctuation lowercase important uppercase looks bad bad bad found multiple spaces like one even worse ones sentences ended without reason others ended randomly inserted middle text students like sir tired said ‚Äúno clean data remove html tags remove punctuation remove stopwords remove numbers remove emojis remove extra spaces ‚Äù one student wrote remove stopwords please please please caps forgot half text lol lol lol used nltk regex resub split join join join functions oh many symbols btw also found lowercase makes text readable readable readable readable readable like like finishing class laughed smiled cleaned cleaned data removed numbers stopwords stopwords stopwords stopwords extra spaces finally dataset looked better better better better big mess mess mess mess mess full tags broken html punctuation text like messy text others like idea anyway cleaning text boring boring boring also fun fun fun makes model smarter faster better clearer slower slower slower said ‚Äúdon‚Äôt forget remove punctuation stopwords numbers emojis html tags‚Äù said ‚Äúyes sir yes sir‚Äù three times three times three times\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "words = text.split()\n",
    "text_no_stop = ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "print(\"After Removing Stopwords:\\n\")\n",
    "print(text_no_stop)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b2864",
   "metadata": {},
   "source": [
    "## **Stemming (Porter Stemmer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c43f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming:\n",
      "\n",
      "sooo yesterday taught student nlp preprocess yeah yeah like lower case stop word punctuat oh wow mess mess student mayb sure anyway talk talk talk comma dot dot dot question mark semicolon stuff said like ‚Äúsir remov punctuat number html tag space space space ‚Äù said make data clean clean clean oh ye ye ye also found sooo mani stopword like oh nooo ugh repeat word everywher everywher everywher sometim text big letter small letter miss letter punctuat punctuat punctuat lowercas import uppercas look bad bad bad found multipl space like one even wors one sentenc end without reason other end randomli insert middl text student like sir tire said ‚Äúno clean data remov html tag remov punctuat remov stopword remov number remov emoji remov extra space ‚Äù one student wrote remov stopword pleas pleas pleas cap forgot half text lol lol lol use nltk regex resub split join join join function oh mani symbol btw also found lowercas make text readabl readabl readabl readabl readabl like like finish class laugh smile clean clean data remov number stopword stopword stopword stopword extra space final dataset look better better better better big mess mess mess mess mess full tag broken html punctuat text like messi text other like idea anyway clean text bore bore bore also fun fun fun make model smarter faster better clearer slower slower slower said ‚Äúdon‚Äôt forget remov punctuat stopword number emoji html tags‚Äù said ‚Äúye sir ye sir‚Äù three time three time three time\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = ' '.join([stemmer.stem(word) for word in text_no_stop.split()])\n",
    "\n",
    "print(\"After Stemming:\\n\")\n",
    "print(stemmed)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba76ba1",
   "metadata": {},
   "source": [
    "## **Lemmatization (better than stemming usually)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6737bda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Lemmatization:\n",
      "\n",
      "sooo yesterday taught student nlp preprocessing yeah yeah like lower case stop word punctuation oh wow mess mess student maybe sure anyway talked talked talked comma dot dot dot question mark semicolon stuff said like ‚Äúsir remove punctuation number html tag space space space ‚Äù said make data clean clean clean oh yes yes yes also found sooo many stopwords like oh nooo ugh repeated word everywhere everywhere everywhere sometimes text big letter small letter missing letter punctuation punctuation punctuation lowercase important uppercase look bad bad bad found multiple space like one even worse one sentence ended without reason others ended randomly inserted middle text student like sir tired said ‚Äúno clean data remove html tag remove punctuation remove stopwords remove number remove emojis remove extra space ‚Äù one student wrote remove stopwords please please please cap forgot half text lol lol lol used nltk regex resub split join join join function oh many symbol btw also found lowercase make text readable readable readable readable readable like like finishing class laughed smiled cleaned cleaned data removed number stopwords stopwords stopwords stopwords extra space finally dataset looked better better better better big mess mess mess mess mess full tag broken html punctuation text like messy text others like idea anyway cleaning text boring boring boring also fun fun fun make model smarter faster better clearer slower slower slower said ‚Äúdon‚Äôt forget remove punctuation stopwords number emojis html tags‚Äù said ‚Äúyes sir yes sir‚Äù three time three time three time\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = ' '.join([lemmatizer.lemmatize(word) for word in text_no_stop.split()])\n",
    "\n",
    "print(\"After Lemmatization:\\n\")\n",
    "print(lemmatized)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1df32",
   "metadata": {},
   "source": [
    "## **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bba5ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words - Feature Names:\n",
      "['also' 'anyway' 'bad' 'better' 'big' 'boring' 'broken' 'btw' 'cap' 'case'\n",
      " 'class' 'clean' 'cleaned' 'cleaning' 'clearer' 'comma' 'data' 'dataset'\n",
      " 'don' 'dot' 'emojis' 'ended' 'even' 'everywhere' 'extra' 'faster'\n",
      " 'finally' 'finishing' 'forget' 'forgot' 'found' 'full' 'fun' 'function'\n",
      " 'half' 'html' 'idea' 'important' 'inserted' 'join' 'laughed' 'letter'\n",
      " 'like' 'lol' 'look' 'looked' 'lower' 'lowercase' 'make' 'many' 'mark'\n",
      " 'maybe' 'mess' 'messy' 'middle' 'missing' 'model' 'multiple' 'nlp' 'nltk'\n",
      " 'no' 'nooo' 'number' 'oh' 'one' 'others' 'please' 'preprocessing'\n",
      " 'punctuation' 'question' 'randomly' 'readable' 'reason' 'regex' 'remove'\n",
      " 'removed' 'repeated' 'resub' 'said' 'semicolon' 'sentence' 'sir' 'slower'\n",
      " 'small' 'smarter' 'smiled' 'sometimes' 'sooo' 'space' 'split' 'stop'\n",
      " 'stopwords' 'student' 'stuff' 'sure' 'symbol' 'tag' 'tags' 'talked'\n",
      " 'taught' 'text' 'three' 'time' 'tired' 'ugh' 'uppercase' 'used' 'without'\n",
      " 'word' 'worse' 'wow' 'wrote' 'yeah' 'yes' 'yesterday']\n",
      "\n",
      "Bag of Words Matrix:\n",
      " [[3 2 3 5 2 3 1 1 1 1 1 4 2 1 1 1 3 1 1 3 2 2 1 3 2 1 1 1 1 1 3 1 3 1 1 4\n",
      "  1 1 1 3 1 3 9 3 1 1 1 2 3 2 1 1 7 1 1 1 1 1 1 1 1 1 4 4 3 2 3 1 8 1 1 5\n",
      "  1 1 9 1 1 1 5 1 1 4 3 1 1 1 1 2 6 1 1 8 4 1 1 1 3 1 3 1 7 3 3 1 1 1 1 1\n",
      "  2 1 1 1 2 5 1]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform([lemmatized])\n",
    "print(\"Bag of Words - Feature Names:\")\n",
    "print(cv.get_feature_names_out())\n",
    "print(\"\\nBag of Words Matrix:\\n\", bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59d118",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d39274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF - Feature Names:\n",
      "['also' 'anyway' 'bad' 'better' 'big' 'boring' 'broken' 'btw' 'cap' 'case'\n",
      " 'class' 'clean' 'cleaned' 'cleaning' 'clearer' 'comma' 'data' 'dataset'\n",
      " 'don' 'dot' 'emojis' 'ended' 'even' 'everywhere' 'extra' 'faster'\n",
      " 'finally' 'finishing' 'forget' 'forgot' 'found' 'full' 'fun' 'function'\n",
      " 'half' 'html' 'idea' 'important' 'inserted' 'join' 'laughed' 'letter'\n",
      " 'like' 'lol' 'look' 'looked' 'lower' 'lowercase' 'make' 'many' 'mark'\n",
      " 'maybe' 'mess' 'messy' 'middle' 'missing' 'model' 'multiple' 'nlp' 'nltk'\n",
      " 'no' 'nooo' 'number' 'oh' 'one' 'others' 'please' 'preprocessing'\n",
      " 'punctuation' 'question' 'randomly' 'readable' 'reason' 'regex' 'remove'\n",
      " 'removed' 'repeated' 'resub' 'said' 'semicolon' 'sentence' 'sir' 'slower'\n",
      " 'small' 'smarter' 'smiled' 'sometimes' 'sooo' 'space' 'split' 'stop'\n",
      " 'stopwords' 'student' 'stuff' 'sure' 'symbol' 'tag' 'tags' 'talked'\n",
      " 'taught' 'text' 'three' 'time' 'tired' 'ugh' 'uppercase' 'used' 'without'\n",
      " 'word' 'worse' 'wow' 'wrote' 'yeah' 'yes' 'yesterday']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.0997 0.0664 0.0997 0.1661 0.0664 0.0997 0.0332 0.0332 0.0332 0.0332\n",
      "  0.0332 0.1329 0.0664 0.0332 0.0332 0.0332 0.0997 0.0332 0.0332 0.0997\n",
      "  0.0664 0.0664 0.0332 0.0997 0.0664 0.0332 0.0332 0.0332 0.0332 0.0332\n",
      "  0.0997 0.0332 0.0997 0.0332 0.0332 0.1329 0.0332 0.0332 0.0332 0.0997\n",
      "  0.0332 0.0997 0.299  0.0997 0.0332 0.0332 0.0332 0.0664 0.0997 0.0664\n",
      "  0.0332 0.0332 0.2326 0.0332 0.0332 0.0332 0.0332 0.0332 0.0332 0.0332\n",
      "  0.0332 0.0332 0.1329 0.1329 0.0997 0.0664 0.0997 0.0332 0.2658 0.0332\n",
      "  0.0332 0.1661 0.0332 0.0332 0.299  0.0332 0.0332 0.0332 0.1661 0.0332\n",
      "  0.0332 0.1329 0.0997 0.0332 0.0332 0.0332 0.0332 0.0664 0.1993 0.0332\n",
      "  0.0332 0.2658 0.1329 0.0332 0.0332 0.0332 0.0997 0.0332 0.0997 0.0332\n",
      "  0.2326 0.0997 0.0997 0.0332 0.0332 0.0332 0.0332 0.0332 0.0664 0.0332\n",
      "  0.0332 0.0332 0.0664 0.1661 0.0332]]\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "tf_matrix = tf.fit_transform([lemmatized])\n",
    "print(\"\\nTF-IDF - Feature Names:\")\n",
    "print(tf.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\\n\", tf_matrix.toarray().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f60d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
