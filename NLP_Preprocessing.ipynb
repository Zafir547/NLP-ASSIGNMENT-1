{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2103370",
   "metadata": {},
   "source": [
    "# **NLP Preprocessing Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cea447",
   "metadata": {},
   "source": [
    "## **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688f95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21179b3",
   "metadata": {},
   "source": [
    "## **Download required NLTK data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e2deabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zafir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zafir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\zafir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86332b9",
   "metadata": {},
   "source": [
    "## **Paragraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ddc5de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Text:\n",
      "\n",
      "\n",
      "Artificial Intelligence is transforming the world rapidly in 2025. \n",
      "Machine learning models are now being used in healthcare, education, finance, \n",
      "and even creative fields like art and music. However, raw text data collected \n",
      "from social media, reviews, articles, and chats is usually very noisy. \n",
      "\n",
      "It contains emojis ðŸ˜ŠðŸš€, hashtags #AI #MachineLearning, URLs https://deeplearningai.com, \n",
      "HTML tags <p></p>, extra spaces   , PUNCTUATION!!!!!, numbers 2025 3.14, and many \n",
      "stop words like the, is, are, and, to, etc. \n",
      "\n",
      "Proper text preprocessing is extremely important because it improves model \n",
      "performance, reduces training time, decreases memory usage, and helps the \n",
      "algorithm focus on meaningful information only. Today we learned several \n",
      "important steps: converting to lowercase, removing numbers, stripping HTML tags, \n",
      "eliminating emojis, cleaning punctuation, removing extra whitespaces, and \n",
      "finally removing English stopwords.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Artificial Intelligence is transforming the world rapidly in 2025. \n",
    "Machine learning models are now being used in healthcare, education, finance, \n",
    "and even creative fields like art and music. However, raw text data collected \n",
    "from social media, reviews, articles, and chats is usually very noisy. \n",
    "\n",
    "It contains emojis ðŸ˜ŠðŸš€, hashtags #AI #MachineLearning, URLs https://deeplearningai.com, \n",
    "HTML tags <p></p>, extra spaces   , PUNCTUATION!!!!!, numbers 2025 3.14, and many \n",
    "stop words like the, is, are, and, to, etc. \n",
    "\n",
    "Proper text preprocessing is extremely important because it improves model \n",
    "performance, reduces training time, decreases memory usage, and helps the \n",
    "algorithm focus on meaningful information only. Today we learned several \n",
    "important steps: converting to lowercase, removing numbers, stripping HTML tags, \n",
    "eliminating emojis, cleaning punctuation, removing extra whitespaces, and \n",
    "finally removing English stopwords.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Orginal Text:\\n\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb1f96",
   "metadata": {},
   "source": [
    "## **Lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0599f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Lowercase:\n",
      " \n",
      "artificial intelligence is transforming the world rapidly in 2025. \n",
      "machine learning models are now being used in healthcare, education, finance, \n",
      "and even creative fields like art and music. however, raw text data collected \n",
      "from social media, reviews, articles, and chats is usually very noisy. \n",
      "\n",
      " ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(\"After Lowercase:\\n\", text[:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d809dc",
   "metadata": {},
   "source": [
    "## **Remove URLs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6195634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'https?://\\S+|www\\.\\s+', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b7c5a",
   "metadata": {},
   "source": [
    "## **Remove HTML tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6da39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'<.*?>', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57399f1f",
   "metadata": {},
   "source": [
    "## **Remove emojis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353999e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = emoji.replace_emoji(text, replace='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a868053a",
   "metadata": {},
   "source": [
    "## **Remove Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de4d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\d+(\\.\\d+)?', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edba0aa",
   "metadata": {},
   "source": [
    "## **Remove Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dbba143",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49da0399",
   "metadata": {},
   "source": [
    "## **Remove Extra Whitespaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d2cfe30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Basic Cleaning:\n",
      "\n",
      "artificial intelligence is transforming the world rapidly in machine learning models are now being used in healthcare education finance and even creative fields like art and music however raw text data collected from social media reviews articles and chats is usually very noisy it contains emojis hashtags ai machinelearning urls html tags extra spaces punctuation numbers and many stop words like the is are and to etc proper text preprocessing is extremely important because it improves model performance reduces training time decreases memory usage and helps the algorithm focus on meaningful information only today we learned several important steps converting to lowercase removing numbers stripping html tags eliminating emojis cleaning punctuation removing extra whitespaces and finally removing english stopwords\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "print(\"After Basic Cleaning:\\n\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2597e",
   "metadata": {},
   "source": [
    "## **Remove Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "891f0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Removing Stopwords:\n",
      "\n",
      "artificial intelligence transforming world rapidly machine learning models used healthcare education finance even creative fields like art music however raw text data collected social media reviews articles chats usually noisy contains emojis hashtags ai machinelearning urls html tags extra spaces punctuation numbers many stop words like etc proper text preprocessing extremely important improves model performance reduces training time decreases memory usage helps algorithm focus meaningful information today learned several important steps converting lowercase removing numbers stripping html tags eliminating emojis cleaning punctuation removing extra whitespaces finally removing english stopwords\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "words = text.split()\n",
    "text_no_stop = ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "print(\"After Removing Stopwords:\\n\")\n",
    "print(text_no_stop)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b2864",
   "metadata": {},
   "source": [
    "## **Stemming (Porter Stemmer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c43f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming:\n",
      "\n",
      "artifici intellig transform world rapidli machin learn model use healthcar educ financ even creativ field like art music howev raw text data collect social media review articl chat usual noisi contain emoji hashtag ai machinelearn url html tag extra space punctuat number mani stop word like etc proper text preprocess extrem import improv model perform reduc train time decreas memori usag help algorithm focu meaning inform today learn sever import step convert lowercas remov number strip html tag elimin emoji clean punctuat remov extra whitespac final remov english stopword\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = ' '.join([stemmer.stem(word) for word in text_no_stop.split()])\n",
    "\n",
    "print(\"After Stemming:\\n\")\n",
    "print(stemmed)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba76ba1",
   "metadata": {},
   "source": [
    "## **Lemmatization (better than stemming usually)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6737bda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Lemmatization:\n",
      "\n",
      "artificial intelligence transforming world rapidly machine learning model used healthcare education finance even creative field like art music however raw text data collected social medium review article chat usually noisy contains emojis hashtags ai machinelearning url html tag extra space punctuation number many stop word like etc proper text preprocessing extremely important improves model performance reduces training time decrease memory usage help algorithm focus meaningful information today learned several important step converting lowercase removing number stripping html tag eliminating emojis cleaning punctuation removing extra whitespaces finally removing english stopwords\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = ' '.join([lemmatizer.lemmatize(word) for word in text_no_stop.split()])\n",
    "\n",
    "print(\"After Lemmatization:\\n\")\n",
    "print(lemmatized)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1df32",
   "metadata": {},
   "source": [
    "## **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bba5ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words - Feature Names:\n",
      "['ai' 'algorithm' 'art' 'article' 'artificial' 'chat' 'cleaning'\n",
      " 'collected' 'contains' 'converting' 'creative' 'data' 'decrease'\n",
      " 'education' 'eliminating' 'emojis' 'english' 'etc' 'even' 'extra'\n",
      " 'extremely' 'field' 'finally' 'finance' 'focus' 'hashtags' 'healthcare'\n",
      " 'help' 'however' 'html' 'important' 'improves' 'information'\n",
      " 'intelligence' 'learned' 'learning' 'like' 'lowercase' 'machine'\n",
      " 'machinelearning' 'many' 'meaningful' 'medium' 'memory' 'model' 'music'\n",
      " 'noisy' 'number' 'performance' 'preprocessing' 'proper' 'punctuation'\n",
      " 'rapidly' 'raw' 'reduces' 'removing' 'review' 'several' 'social' 'space'\n",
      " 'step' 'stop' 'stopwords' 'stripping' 'tag' 'text' 'time' 'today'\n",
      " 'training' 'transforming' 'url' 'usage' 'used' 'usually' 'whitespaces'\n",
      " 'word' 'world']\n",
      "\n",
      "Bag of Words Matrix:\n",
      " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1\n",
      "  2 1 1 1 1 1 1 1 2 1 1 2 1 1 1 2 1 1 1 3 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1\n",
      "  1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform([lemmatized])\n",
    "print(\"Bag of Words - Feature Names:\")\n",
    "print(cv.get_feature_names_out())\n",
    "print(\"\\nBag of Words Matrix:\\n\", bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59d118",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d39274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF - Feature Names:\n",
      "['ai' 'algorithm' 'art' 'article' 'artificial' 'chat' 'cleaning'\n",
      " 'collected' 'contains' 'converting' 'creative' 'data' 'decrease'\n",
      " 'education' 'eliminating' 'emojis' 'english' 'etc' 'even' 'extra'\n",
      " 'extremely' 'field' 'finally' 'finance' 'focus' 'hashtags' 'healthcare'\n",
      " 'help' 'however' 'html' 'important' 'improves' 'information'\n",
      " 'intelligence' 'learned' 'learning' 'like' 'lowercase' 'machine'\n",
      " 'machinelearning' 'many' 'meaningful' 'medium' 'memory' 'model' 'music'\n",
      " 'noisy' 'number' 'performance' 'preprocessing' 'proper' 'punctuation'\n",
      " 'rapidly' 'raw' 'reduces' 'removing' 'review' 'several' 'social' 'space'\n",
      " 'step' 'stop' 'stopwords' 'stripping' 'tag' 'text' 'time' 'today'\n",
      " 'training' 'transforming' 'url' 'usage' 'used' 'usually' 'whitespaces'\n",
      " 'word' 'world']\n",
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.0933 0.0933 0.0933 0.0933 0.0933 0.0933 0.0933 0.0933 0.0933 0.0933\n",
      "  0.0933 0.0933 0.0933 0.0933 0.0933 0.1865 0.0933 0.0933 0.0933 0.1865\n",
      "  0.0933 0.0933 0.0933 0.0933 0.0933 0.0933 0.0933 0.0933 0.0933 0.1865\n",
      "  0.1865 0.0933 0.0933 0.0933 0.0933 0.0933 0.1865 0.0933 0.0933 0.0933\n",
      "  0.0933 0.0933 0.0933 0.0933 0.1865 0.0933 0.0933 0.1865 0.0933 0.0933\n",
      "  0.0933 0.1865 0.0933 0.0933 0.0933 0.2798 0.0933 0.0933 0.0933 0.0933\n",
      "  0.0933 0.0933 0.0933 0.0933 0.1865 0.1865 0.0933 0.0933 0.0933 0.0933\n",
      "  0.0933 0.0933 0.0933 0.0933 0.0933 0.0933 0.0933]]\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer()\n",
    "tf_matrix = tf.fit_transform([lemmatized])\n",
    "print(\"\\nTF-IDF - Feature Names:\")\n",
    "print(tf.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\\n\", tf_matrix.toarray().round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
